[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:61234 (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:61234 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:61233 (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:61233 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:61232 (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:61232 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:61231 (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:61231 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:61230 (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:61230 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:61229 (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:61229 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to [::]:61228 (errno: 98 - Address already in use).
[W socket.cpp:426] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:462] [c10d] The server socket has failed to listen on any local network address.
The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:61228 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
Unsupported operator aten::mul encountered 33 time(s)
Unsupported operator aten::add encountered 60 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
============ tiny ===================
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmtiny/ckpt_epoch_292.pth
dict_items([('conv', 0.003612672), ('layer_norm', 0.0023808), ('linear', 0.226492416), ('einsum', 0.069988), ('PythonOp.SelectiveScanFn', 0.061637828)])
GFlops:  0.364111716 Params:  22124448
======== model vssm img_size 64 params 22124448 flops 0.364111716
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmtiny/ckpt_epoch_292.pth
Warning, x.shape torch.Size([1, 7, 7, 384]) is not match even ===========
Unsupported operator aten::mul encountered 33 time(s)
Unsupported operator aten::add encountered 60 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Warning, x.shape torch.Size([1, 7, 7, 384]) is not match even ===========
Warning, x.shape torch.Size([1, 7, 7, 384]) is not match even ===========
dict_items([('conv', 0.010973952), ('layer_norm', 0.00717888), ('linear', 0.666796032), ('einsum', 0.2092375), ('PythonOp.SelectiveScanFn', 0.18648843)])
GFlops:  1.0806747939999999 Params:  22124448
======== model vssm img_size 112 params 22124448 flops 1.0806747939999999
Unsupported operator aten::mul encountered 33 time(s)
Unsupported operator aten::add encountered 60 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmtiny/ckpt_epoch_292.pth
dict_items([('conv', 0.044255232), ('layer_norm', 0.0291648), ('linear', 2.774532096), ('einsum', 0.8574), ('PythonOp.SelectiveScanFn', 0.755079368)])
GFlops:  4.460431496 Params:  22124448
======== model vssm img_size 224 params 22124448 flops 4.460431496
Unsupported operator aten::mul encountered 33 time(s)
Unsupported operator aten::add encountered 60 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmtiny/ckpt_epoch_292.pth
dict_items([('conv', 0.130056192), ('layer_norm', 0.0857088), ('linear', 8.153726976), ('einsum', 2.519775), ('PythonOp.SelectiveScanFn', 2.218832408)])
GFlops:  13.108099376000002 Params:  22124448
======== model vssm img_size 384 params 22124448 flops 13.108099376000002
Unsupported operator aten::mul encountered 33 time(s)
Unsupported operator aten::add encountered 60 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmtiny/ckpt_epoch_292.pth
dict_items([('conv', 0.231211008), ('layer_norm', 0.1523712), ('linear', 14.495514624), ('einsum', 4.47945), ('PythonOp.SelectiveScanFn', 3.944970392)])
GFlops:  23.303517224000004 Params:  22124448
======== model vssm img_size 512 params 22124448 flops 23.303517224000004
Unsupported operator aten::mul encountered 33 time(s)
Unsupported operator aten::add encountered 60 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmtiny/ckpt_epoch_292.pth
dict_items([('conv', 0.3612672), ('layer_norm', 0.23808), ('linear', 22.6492416), ('einsum', 6.9988), ('PythonOp.SelectiveScanFn', 6.1637828)])
GFlops:  36.4111716 Params:  22124448
======== model vssm img_size 640 params 22124448 flops 36.4111716
Unsupported operator aten::mul encountered 33 time(s)
Unsupported operator aten::add encountered 60 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmtiny/ckpt_epoch_292.pth
dict_items([('conv', 0.520224768), ('layer_norm', 0.3428352), ('linear', 32.614907904), ('einsum', 10.0791), ('PythonOp.SelectiveScanFn', 8.875599632)])
GFlops:  52.43266750400001 Params:  22124448
======== model vssm img_size 768 params 22124448 flops 52.43266750400001
Unsupported operator aten::mul encountered 33 time(s)
Unsupported operator aten::add encountered 60 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmtiny/ckpt_epoch_292.pth
dict_items([('conv', 0.924844032), ('layer_norm', 0.6094848), ('linear', 57.982058496), ('einsum', 17.9175), ('PythonOp.SelectiveScanFn', 15.779641568)])
GFlops:  93.213528896 Params:  22124448
======== model vssm img_size 1024 params 22124448 flops 93.213528896
Unsupported operator aten::mul encountered 33 time(s)
Unsupported operator aten::add encountered 60 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmtiny/ckpt_epoch_292.pth
dict_items([('conv', 1.1063808), ('layer_norm', 0.72912), ('linear', 69.3633024), ('einsum', 21.43475), ('PythonOp.SelectiveScanFn', 18.8764592)])
GFlops:  111.5100124 Params:  22124448
======== model vssm img_size 1120 params 22124448 flops 111.5100124
Unsupported operator aten::mul encountered 33 time(s)
Unsupported operator aten::add encountered 60 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmtiny/ckpt_epoch_292.pth
dict_items([('conv', 1.4450688), ('layer_norm', 0.95232), ('linear', 90.5969664), ('einsum', 27.99675), ('PythonOp.SelectiveScanFn', 24.6546212)])
GFlops:  145.6457264 Params:  22124448
======== model vssm img_size 1280 params 22124448 flops 145.6457264
01/17 10:59:17 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 10:59:17 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
01/17 10:59:18 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 10:59:18 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
01/17 10:59:19 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 10:59:19 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
Unsupported operator aten::rsub encountered 24 time(s)
Unsupported operator aten::pad encountered 12 time(s)
Unsupported operator aten::mul encountered 24 time(s)
Unsupported operator aten::add encountered 53 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
Unsupported operator aten::fill_ encountered 45 time(s)
Unsupported operator aten::sub encountered 17 time(s)
Unsupported operator aten::ne encountered 5 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.014450688), ('layer_norm', 0.01862784), ('linear', 4.3352064), ('matmul', 0.140141568)])
GFlops:  4.508426495999999 Params:  28288354
======== model swin img_size 224 params 28288354 flops 4.508426495999999
01/17 10:59:22 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 10:59:23 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
Unsupported operator aten::rsub encountered 24 time(s)
Unsupported operator aten::pad encountered 12 time(s)
Unsupported operator aten::mul encountered 24 time(s)
Unsupported operator aten::add encountered 54 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
Unsupported operator aten::fill_ encountered 54 time(s)
Unsupported operator aten::sub encountered 18 time(s)
Unsupported operator aten::ne encountered 6 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.042467328), ('layer_norm', 0.05474304), ('linear', 13.778878464), ('matmul', 0.477587712)])
GFlops:  14.353676544 Params:  28288354
======== model swin img_size 384 params 28288354 flops 14.353676544
01/17 10:59:28 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 10:59:28 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
Unsupported operator aten::rsub encountered 24 time(s)
Unsupported operator aten::pad encountered 12 time(s)
Unsupported operator aten::mul encountered 24 time(s)
Unsupported operator aten::add encountered 54 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
Unsupported operator aten::fill_ encountered 54 time(s)
Unsupported operator aten::sub encountered 18 time(s)
Unsupported operator aten::ne encountered 6 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.075497472), ('layer_norm', 0.09732096), ('linear', 24.566833152), ('matmul', 0.860211072)])
GFlops:  25.599862656 Params:  28288354
======== model swin img_size 512 params 28288354 flops 25.599862656
01/17 10:59:32 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 10:59:32 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
Unsupported operator aten::rsub encountered 24 time(s)
Unsupported operator aten::pad encountered 12 time(s)
Unsupported operator aten::mul encountered 24 time(s)
Unsupported operator aten::add encountered 54 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
Unsupported operator aten::fill_ encountered 54 time(s)
Unsupported operator aten::sub encountered 18 time(s)
Unsupported operator aten::ne encountered 6 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.1179648), ('layer_norm', 0.152064), ('linear', 36.380418048), ('matmul', 1.217940864)])
GFlops:  37.868387712 Params:  28288354
======== model swin img_size 640 params 28288354 flops 37.868387712
01/17 10:59:37 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 10:59:38 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
Unsupported operator aten::rsub encountered 24 time(s)
Unsupported operator aten::pad encountered 12 time(s)
Unsupported operator aten::mul encountered 24 time(s)
Unsupported operator aten::add encountered 54 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
Unsupported operator aten::fill_ encountered 54 time(s)
Unsupported operator aten::sub encountered 18 time(s)
Unsupported operator aten::ne encountered 6 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.169869312), ('layer_norm', 0.21897216), ('linear', 52.514390016), ('matmul', 1.744393728)])
GFlops:  54.647625216 Params:  28288354
======== model swin img_size 768 params 28288354 flops 54.647625216
01/17 10:59:44 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 10:59:44 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
Unsupported operator aten::rsub encountered 24 time(s)
Unsupported operator aten::pad encountered 12 time(s)
Unsupported operator aten::mul encountered 24 time(s)
Unsupported operator aten::add encountered 54 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
Unsupported operator aten::fill_ encountered 54 time(s)
Unsupported operator aten::sub encountered 18 time(s)
Unsupported operator aten::ne encountered 6 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.301989888), ('layer_norm', 0.38928384), ('linear', 94.889484288), ('matmul', 3.218646144)])
GFlops:  98.79940416000001 Params:  28288354
======== model swin img_size 1024 params 28288354 flops 98.79940416000001
01/17 10:59:49 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 10:59:50 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
Unsupported operator aten::rsub encountered 24 time(s)
Unsupported operator aten::pad encountered 12 time(s)
Unsupported operator aten::mul encountered 24 time(s)
Unsupported operator aten::add encountered 54 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
Unsupported operator aten::fill_ encountered 54 time(s)
Unsupported operator aten::sub encountered 18 time(s)
Unsupported operator aten::ne encountered 6 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.3612672), ('layer_norm', 0.465696), ('linear', 108.38016), ('matmul', 3.5035392)])
GFlops:  112.7106624 Params:  28288354
======== model swin img_size 1120 params 28288354 flops 112.7106624
01/17 10:59:56 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 10:59:57 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_tiny_224_b16x64_300e_imagenet_20210616_090925-66df6be6.pth
Unsupported operator aten::rsub encountered 24 time(s)
Unsupported operator aten::pad encountered 12 time(s)
Unsupported operator aten::mul encountered 24 time(s)
Unsupported operator aten::add encountered 54 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
Unsupported operator aten::fill_ encountered 54 time(s)
Unsupported operator aten::sub encountered 18 time(s)
Unsupported operator aten::ne encountered 6 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.4718592), ('layer_norm', 0.608256), ('linear', 144.842489856), ('matmul', 4.78509696)])
GFlops:  150.70770201599998 Params:  28288354
======== model swin img_size 1280 params 28288354 flops 150.70770201599998
01/17 11:00:01 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:02 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_32xb128_in1k_20221207-998cf3e9.pth
Unsupported operator aten::gelu encountered 18 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 18 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.02391552), ('layer_norm', 0.0012288), ('linear', 0.339738624)])
GFlops:  0.364882944 Params:  28589128
======== model convnext img_size 64 params 28589128 flops 0.364882944
01/17 11:00:03 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:04 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_32xb128_in1k_20221207-998cf3e9.pth
Unsupported operator aten::gelu encountered 18 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 18 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.069040512), ('layer_norm', 0.00371328), ('linear', 0.994443264)])
GFlops:  1.0671970560000001 Params:  28589128
======== model convnext img_size 112 params 28589128 flops 1.0671970560000001
01/17 11:00:05 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:05 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_32xb128_in1k_20221207-998cf3e9.pth
Unsupported operator aten::gelu encountered 18 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 18 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.29296512), ('layer_norm', 0.0150528), ('linear', 4.161798144)])
GFlops:  4.469816064 Params:  28589128
======== model convnext img_size 224 params 28589128 flops 4.469816064
01/17 11:00:08 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:09 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_32xb128_in1k_20221207-998cf3e9.pth
Unsupported operator aten::gelu encountered 18 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 18 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.86095872), ('layer_norm', 0.0442368), ('linear', 12.230590464)])
GFlops:  13.135785984 Params:  28589128
======== model convnext img_size 384 params 28589128 flops 13.135785984
01/17 11:00:11 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:12 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_32xb128_in1k_20221207-998cf3e9.pth
Unsupported operator aten::gelu encountered 18 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 18 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 1.53059328), ('layer_norm', 0.0786432), ('linear', 21.743271936)])
GFlops:  23.352508416 Params:  28589128
======== model convnext img_size 512 params 28589128 flops 23.352508416
01/17 11:00:14 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:15 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_32xb128_in1k_20221207-998cf3e9.pth
Unsupported operator aten::gelu encountered 18 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 18 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 2.391552), ('layer_norm', 0.12288), ('linear', 33.9738624)])
GFlops:  36.4882944 Params:  28589128
======== model convnext img_size 640 params 28589128 flops 36.4882944
01/17 11:00:18 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:18 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_32xb128_in1k_20221207-998cf3e9.pth
Unsupported operator aten::gelu encountered 18 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 18 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 3.44383488), ('layer_norm', 0.1769472), ('linear', 48.922361856)])
GFlops:  52.543143936 Params:  28589128
======== model convnext img_size 768 params 28589128 flops 52.543143936
01/17 11:00:21 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:22 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_32xb128_in1k_20221207-998cf3e9.pth
Unsupported operator aten::gelu encountered 18 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 18 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 6.12237312), ('layer_norm', 0.3145728), ('linear', 86.973087744)])
GFlops:  93.410033664 Params:  28589128
======== model convnext img_size 1024 params 28589128 flops 93.410033664
01/17 11:00:24 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:24 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_32xb128_in1k_20221207-998cf3e9.pth
Unsupported operator aten::gelu encountered 18 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 18 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 7.324128), ('layer_norm', 0.37632), ('linear', 104.0449536)])
GFlops:  111.7454016 Params:  28589128
======== model convnext img_size 1120 params 28589128 flops 111.7454016
01/17 11:00:26 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:27 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-tiny_32xb128_in1k_20221207-998cf3e9.pth
Unsupported operator aten::gelu encountered 18 time(s)
Unsupported operator aten::mul encountered 18 time(s)
Unsupported operator aten::add encountered 18 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 9.566208), ('layer_norm', 0.49152), ('linear', 135.8954496)])
GFlops:  145.9531776 Params:  28589128
======== model convnext img_size 1280 params 28589128 flops 145.9531776
01/17 11:00:29 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:30 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.004718592), ('layer_norm', 0.000816), ('linear', 0.360972288), ('matmul', 0.002663424)])
GFlops:  0.369170304 Params:  22050664
======== model deit img_size 64 params 22050664 flops 0.369170304
01/17 11:00:32 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:32 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.014450688), ('layer_norm', 0.0024), ('linear', 1.0616832), ('matmul', 0.02304)])
GFlops:  1.101573888 Params:  22050664
======== model deit img_size 112 params 22050664 flops 1.101573888
01/17 11:00:34 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:35 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb.pth
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.057802752), ('layer_norm', 0.009456), ('linear', 4.183031808), ('matmul', 0.357663744)])
GFlops:  4.607954304 Params:  22050664
======== model deit img_size 224 params 22050664 flops 4.607954304
01/17 11:00:37 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:38 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.169869312), ('layer_norm', 0.027696), ('linear', 12.251824128), ('matmul', 3.068273664)])
GFlops:  15.517663104 Params:  22050664
======== model deit img_size 384 params 22050664 flops 15.517663104
01/17 11:00:39 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:40 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.301989888), ('layer_norm', 0.0492), ('linear', 21.7645056), ('matmul', 9.68256)])
GFlops:  31.798255488000002 Params:  22050664
======== model deit img_size 512 params 22050664 flops 31.798255488000002
01/17 11:00:43 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:43 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.4718592), ('layer_norm', 0.076848), ('linear', 33.995096064), ('matmul', 23.622460416)])
GFlops:  58.16626368 Params:  22050664
======== model deit img_size 640 params 22050664 flops 58.16626368
01/17 11:00:46 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:46 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.679477248), ('layer_norm', 0.11064), ('linear', 48.94359552), ('matmul', 48.9648384)])
GFlops:  98.698551168 Params:  22050664
======== model deit img_size 768 params 22050664 flops 98.698551168
01/17 11:00:49 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:50 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 1.207959552), ('layer_norm', 0.196656), ('linear', 86.994321408), ('matmul', 154.694329344)])
GFlops:  243.093266304 Params:  22050664
======== model deit img_size 1024 params 22050664 flops 243.093266304
01/17 11:00:53 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:54 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 1.4450688), ('layer_norm', 0.235248), ('linear', 104.066187264), ('matmul', 221.366486016)])
GFlops:  327.11299008000003 Params:  22050664
======== model deit img_size 1120 params 22050664 flops 327.11299008000003
01/17 11:00:56 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:00:57 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-small_pt-4xb256_in1k_20220218-9425b9bb.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 1.8874368), ('layer_norm', 0.307248), ('linear', 135.916683264), ('matmul', 377.605334016)])
GFlops:  515.71670208 Params:  22050664
======== model deit img_size 1280 params 22050664 flops 515.71670208
01/17 11:01:00 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 0.333643776), ('batch_norm', 0.001814528)])
GFlops:  0.335458304 Params:  25557032
======== model resnet img_size 64 params 25557032 flops 0.335458304
01/17 11:01:02 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 1.075851264), ('batch_norm', 0.005637632)])
GFlops:  1.081488896 Params:  25557032
======== model resnet img_size 112 params 25557032 flops 1.081488896
01/17 11:01:05 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 4.087136256), ('batch_norm', 0.022227968)])
GFlops:  4.109364224 Params:  25557032
======== model resnet img_size 224 params 25557032 flops 4.109364224
01/17 11:01:07 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 12.011175936), ('batch_norm', 0.065323008)])
GFlops:  12.076498944 Params:  25557032
======== model resnet img_size 384 params 25557032 flops 12.076498944
01/17 11:01:09 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 21.353201664), ('batch_norm', 0.116129792)])
GFlops:  21.469331456 Params:  25557032
======== model resnet img_size 512 params 25557032 flops 21.469331456
01/17 11:01:11 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 33.3643776), ('batch_norm', 0.1814528)])
GFlops:  33.5458304 Params:  25557032
======== model resnet img_size 640 params 25557032 flops 33.5458304
01/17 11:01:13 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 48.044703744), ('batch_norm', 0.261292032)])
GFlops:  48.305995776 Params:  25557032
======== model resnet img_size 768 params 25557032 flops 48.305995776
01/17 11:01:15 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 85.412806656), ('batch_norm', 0.464519168)])
GFlops:  85.877325824 Params:  25557032
======== model resnet img_size 1024 params 25557032 flops 85.877325824
01/17 11:01:17 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 102.1784064), ('batch_norm', 0.5556992)])
GFlops:  102.7341056 Params:  25557032
======== model resnet img_size 1120 params 25557032 flops 102.7341056
01/17 11:01:19 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet50_8xb32_in1k_20210831-ea4938fc.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 16 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 133.4575104), ('batch_norm', 0.7258112)])
GFlops:  134.1833216 Params:  25557032
======== model resnet img_size 1280 params 25557032 flops 134.1833216
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
============ small ===================
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmsmall/ema_ckpt_epoch_238.pth
dict_items([('conv', 0.005603328), ('layer_norm', 0.00403968), ('linear', 0.481296384), ('einsum', 0.140764), ('PythonOp.SelectiveScanFn', 0.11206814)])
GFlops:  0.743771532 Params:  43648416
======== model vssm img_size 64 params 43648416 flops 0.743771532
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmsmall/ema_ckpt_epoch_238.pth
Warning, x.shape torch.Size([1, 7, 7, 384]) is not match even ===========
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Warning, x.shape torch.Size([1, 7, 7, 384]) is not match even ===========
Warning, x.shape torch.Size([1, 7, 7, 384]) is not match even ===========
dict_items([('conv', 0.017070336), ('layer_norm', 0.0122592), ('linear', 1.447133184), ('einsum', 0.4260025), ('PythonOp.SelectiveScanFn', 0.340925298)])
GFlops:  2.243390518 Params:  43648416
======== model vssm img_size 112 params 43648416 flops 2.243390518
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmsmall/ema_ckpt_epoch_238.pth
dict_items([('conv', 0.068640768), ('layer_norm', 0.04948608), ('linear', 5.895880704), ('einsum', 1.72446), ('PythonOp.SelectiveScanFn', 1.37286284)])
GFlops:  9.111330392 Params:  43648416
======== model vssm img_size 224 params 43648416 flops 9.111330392
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmsmall/ema_ckpt_epoch_238.pth
dict_items([('conv', 0.201719808), ('layer_norm', 0.14542848), ('linear', 17.326669824), ('einsum', 5.067945), ('PythonOp.SelectiveScanFn', 4.03424804)])
GFlops:  26.776011152000002 Params:  43648416
======== model vssm img_size 384 params 43648416 flops 26.776011152000002
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmsmall/ema_ckpt_epoch_238.pth
dict_items([('conv', 0.358612992), ('layer_norm', 0.25853952), ('linear', 30.802968576), ('einsum', 9.00915), ('PythonOp.SelectiveScanFn', 7.17276596)])
GFlops:  47.602037048 Params:  43648416
======== model vssm img_size 512 params 43648416 flops 47.602037048
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmsmall/ema_ckpt_epoch_238.pth
dict_items([('conv', 0.5603328), ('layer_norm', 0.403968), ('linear', 48.1296384), ('einsum', 14.0764), ('PythonOp.SelectiveScanFn', 11.206814)])
GFlops:  74.3771532 Params:  43648416
======== model vssm img_size 640 params 43648416 flops 74.3771532
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmsmall/ema_ckpt_epoch_238.pth
dict_items([('conv', 0.806879232), ('layer_norm', 0.58171392), ('linear', 69.306679296), ('einsum', 20.2707), ('PythonOp.SelectiveScanFn', 16.13708216)])
GFlops:  107.10305460800001 Params:  43648416
======== model vssm img_size 768 params 43648416 flops 107.10305460800001
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmsmall/ema_ckpt_epoch_238.pth
dict_items([('conv', 1.434451968), ('layer_norm', 1.03415808), ('linear', 123.211874304), ('einsum', 36.0345), ('PythonOp.SelectiveScanFn', 28.68974384)])
GFlops:  190.40472819200002 Params:  43648416
======== model vssm img_size 1024 params 43648416 flops 190.40472819200002
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmsmall/ema_ckpt_epoch_238.pth
dict_items([('conv', 1.7160192), ('layer_norm', 1.237152), ('linear', 147.3970176), ('einsum', 43.11125), ('PythonOp.SelectiveScanFn', 34.320146)])
GFlops:  227.7815848 Params:  43648416
======== model vssm img_size 1120 params 43648416 flops 227.7815848
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmsmall/ema_ckpt_epoch_238.pth
dict_items([('conv', 2.2413312), ('layer_norm', 1.615872), ('linear', 192.5185536), ('einsum', 56.30805), ('PythonOp.SelectiveScanFn', 44.826206)])
GFlops:  297.51001279999997 Params:  43648416
======== model vssm img_size 1280 params 43648416 flops 297.51001279999997
01/17 11:04:40 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:04:42 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth
01/17 11:04:44 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:04:46 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth
01/17 11:04:49 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:04:51 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 95 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 99 time(s)
Unsupported operator aten::sub encountered 23 time(s)
Unsupported operator aten::ne encountered 11 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.014450688), ('layer_norm', 0.02765952), ('linear', 8.497004544), ('matmul', 0.228652032)])
GFlops:  8.767766783999999 Params:  49606258
======== model swin img_size 224 params 49606258 flops 8.767766783999999
01/17 11:05:06 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:05:08 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.042467328), ('layer_norm', 0.08128512), ('linear', 27.481669632), ('matmul', 0.831629568)])
GFlops:  28.437051648 Params:  49606258
======== model swin img_size 384 params 49606258 flops 28.437051648
01/17 11:05:25 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:05:27 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.075497472), ('layer_norm', 0.14450688), ('linear', 47.732760576), ('matmul', 1.413401472)])
GFlops:  49.36616639999999 Params:  49606258
======== model swin img_size 512 params 49606258 flops 49.36616639999999
01/17 11:05:39 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:05:39 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.1179648), ('layer_norm', 0.225792), ('linear', 71.51505408), ('matmul', 2.01453504)])
GFlops:  73.87334591999999 Params:  49606258
======== model swin img_size 640 params 49606258 flops 73.87334591999999
01/17 11:05:50 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:05:51 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.169869312), ('layer_norm', 0.32514048), ('linear', 102.123307008), ('matmul', 2.828646912)])
GFlops:  105.446963712 Params:  49606258
======== model swin img_size 768 params 49606258 flops 105.446963712
01/17 11:06:00 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:06:01 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.301989888), ('layer_norm', 0.57802752), ('linear', 187.553193984), ('matmul', 5.431407744)])
GFlops:  193.864619136 Params:  49606258
======== model swin img_size 1024 params 49606258 flops 193.864619136
01/17 11:06:13 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:06:16 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.3612672), ('layer_norm', 0.691488), ('linear', 212.4251136), ('matmul', 5.7163008)])
GFlops:  219.1941696 Params:  49606258
======== model swin img_size 1120 params 49606258 flops 219.1941696
01/17 11:06:28 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:06:30 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_small_224_b16x64_300e_imagenet_20210615_110219-7f9d988b.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.4718592), ('layer_norm', 0.903168), ('linear', 285.381033984), ('matmul', 7.971473664)])
GFlops:  294.72753484799995 Params:  49606258
======== model swin img_size 1280 params 49606258 flops 294.72753484799995
01/17 11:06:48 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:06:50 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_32xb128_in1k_20221207-4ab7052c.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.029334528), ('layer_norm', 0.00178176), ('linear', 0.679477248)])
GFlops:  0.710593536 Params:  50223688
======== model convnext img_size 64 params 50223688 flops 0.710593536
01/17 11:06:54 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:06:55 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_32xb128_in1k_20221207-4ab7052c.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.085636224), ('layer_norm', 0.00540672), ('linear', 2.0348928)])
GFlops:  2.125935744 Params:  50223688
======== model convnext img_size 112 params 50223688 flops 2.125935744
01/17 11:06:58 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:06:59 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_32xb128_in1k_20221207-4ab7052c.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.359347968), ('layer_norm', 0.02182656), ('linear', 8.323596288)])
GFlops:  8.704770816 Params:  50223688
======== model convnext img_size 224 params 50223688 flops 8.704770816
01/17 11:07:03 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:07:05 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_32xb128_in1k_20221207-4ab7052c.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 1.056043008), ('layer_norm', 0.06414336), ('linear', 24.461180928)])
GFlops:  25.581367296 Params:  50223688
======== model convnext img_size 384 params 50223688 flops 25.581367296
01/17 11:07:11 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:07:13 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_32xb128_in1k_20221207-4ab7052c.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 1.877409792), ('layer_norm', 0.11403264), ('linear', 43.486543872)])
GFlops:  45.477986304 Params:  50223688
======== model convnext img_size 512 params 50223688 flops 45.477986304
01/17 11:07:18 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:07:20 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_32xb128_in1k_20221207-4ab7052c.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 2.9334528), ('layer_norm', 0.178176), ('linear', 67.9477248)])
GFlops:  71.05935360000001 Params:  50223688
======== model convnext img_size 640 params 50223688 flops 71.05935360000001
01/17 11:07:25 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:07:27 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_32xb128_in1k_20221207-4ab7052c.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 4.224172032), ('layer_norm', 0.25657344), ('linear', 97.844723712)])
GFlops:  102.325469184 Params:  50223688
======== model convnext img_size 768 params 50223688 flops 102.325469184
01/17 11:07:32 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:07:34 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_32xb128_in1k_20221207-4ab7052c.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 7.509639168), ('layer_norm', 0.45613056), ('linear', 173.946175488)])
GFlops:  181.911945216 Params:  50223688
======== model convnext img_size 1024 params 50223688 flops 181.911945216
01/17 11:07:39 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:07:40 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_32xb128_in1k_20221207-4ab7052c.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 8.9836992), ('layer_norm', 0.545664), ('linear', 208.0899072)])
GFlops:  217.6192704 Params:  50223688
======== model convnext img_size 1120 params 50223688 flops 217.6192704
01/17 11:07:45 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:07:46 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-small_32xb128_in1k_20221207-4ab7052c.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 11.7338112), ('layer_norm', 0.712704), ('linear', 271.7908992)])
GFlops:  284.23741440000003 Params:  50223688
======== model convnext img_size 1280 params 50223688 flops 284.23741440000003
01/17 11:07:51 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:07:53 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.009437184), ('layer_norm', 0.001632), ('linear', 1.443889152), ('matmul', 0.005326848)])
GFlops:  1.4602851839999997 Params:  86567656
======== model deit img_size 64 params 86567656 flops 1.4602851839999997
01/17 11:07:57 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:07:59 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.028901376), ('layer_norm', 0.0048), ('linear', 4.2467328), ('matmul', 0.04608)])
GFlops:  4.326514176 Params:  86567656
======== model deit img_size 112 params 86567656 flops 4.326514176
01/17 11:08:01 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:08:04 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c.pth
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.115605504), ('layer_norm', 0.018912), ('linear', 16.732127232), ('matmul', 0.715327488)])
GFlops:  17.581972224 Params:  86567656
======== model deit img_size 224 params 86567656 flops 17.581972224
01/17 11:08:07 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:08:10 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.339738624), ('layer_norm', 0.055392), ('linear', 49.007296512), ('matmul', 6.136547328)])
GFlops:  55.538974464 Params:  86567656
======== model deit img_size 384 params 86567656 flops 55.538974464
01/17 11:08:14 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:08:17 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.603979776), ('layer_norm', 0.0984), ('linear', 87.0580224), ('matmul', 19.36512)])
GFlops:  107.125522176 Params:  86567656
======== model deit img_size 512 params 86567656 flops 107.125522176
01/17 11:08:20 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:08:23 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.9437184), ('layer_norm', 0.153696), ('linear', 135.980384256), ('matmul', 47.244920832)])
GFlops:  184.322719488 Params:  86567656
======== model deit img_size 640 params 86567656 flops 184.322719488
01/17 11:08:26 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:08:28 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 1.358954496), ('layer_norm', 0.22128), ('linear', 195.77438208), ('matmul', 97.9296768)])
GFlops:  295.284293376 Params:  86567656
======== model deit img_size 768 params 86567656 flops 295.284293376
01/17 11:08:33 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:08:34 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 2.415919104), ('layer_norm', 0.393312), ('linear', 347.977285632), ('matmul', 309.388658688)])
GFlops:  660.175175424 Params:  86567656
======== model deit img_size 1024 params 86567656 flops 660.175175424
01/17 11:08:36 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:08:37 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 2.8901376), ('layer_norm', 0.470496), ('linear', 416.264749056), ('matmul', 442.732972032)])
GFlops:  862.3583546880001 Params:  86567656
======== model deit img_size 1120 params 86567656 flops 862.3583546880001
01/17 11:08:41 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:08:44 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/deit/deit-base_pt-16xb64_in1k_20220216-db63c16c.pth
Unsupported operator aten::upsample_bicubic2d encountered 1 time(s)
Unsupported operator aten::add encountered 25 time(s)
Unsupported operator aten::pow encountered 12 time(s)
Unsupported operator aten::div encountered 12 time(s)
Unsupported operator aten::softmax encountered 12 time(s)
Unsupported operator aten::gelu encountered 12 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.layers.0.attn.out_drop, backbone.layers.0.ffn.dropout_layer, backbone.layers.1.attn.out_drop, backbone.layers.1.ffn.dropout_layer, backbone.layers.10.attn.out_drop, backbone.layers.10.ffn.dropout_layer, backbone.layers.11.attn.out_drop, backbone.layers.11.ffn.dropout_layer, backbone.layers.2.attn.out_drop, backbone.layers.2.ffn.dropout_layer, backbone.layers.3.attn.out_drop, backbone.layers.3.ffn.dropout_layer, backbone.layers.4.attn.out_drop, backbone.layers.4.ffn.dropout_layer, backbone.layers.5.attn.out_drop, backbone.layers.5.ffn.dropout_layer, backbone.layers.6.attn.out_drop, backbone.layers.6.ffn.dropout_layer, backbone.layers.7.attn.out_drop, backbone.layers.7.ffn.dropout_layer, backbone.layers.8.attn.out_drop, backbone.layers.8.ffn.dropout_layer, backbone.layers.9.attn.out_drop, backbone.layers.9.ffn.dropout_layer, backbone.patch_embed.adaptive_padding, data_preprocessor, head, head.layers, head.layers.head, head.loss_module, head.loss_module.ce
dict_items([('conv', 3.7748736), ('layer_norm', 0.614496), ('linear', 543.666733056), ('matmul', 755.210668032)])
GFlops:  1303.266770688 Params:  86567656
======== model deit img_size 1280 params 86567656 flops 1303.266770688
01/17 11:08:47 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 33 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 0.63668224), ('batch_norm', 0.002650112)])
GFlops:  0.6393323519999999 Params:  44549160
======== model resnet img_size 64 params 44549160 flops 0.6393323519999999
01/17 11:08:55 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 33 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 2.00390656), ('batch_norm', 0.008196608)])
GFlops:  2.012103168 Params:  44549160
======== model resnet img_size 112 params 44549160 flops 2.012103168
01/17 11:09:03 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 33 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 7.79935744), ('batch_norm', 0.032463872)])
GFlops:  7.831821312 Params:  44549160
======== model resnet img_size 224 params 44549160 flops 7.831821312
01/17 11:09:11 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 33 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 22.92056064), ('batch_norm', 0.095404032)])
GFlops:  23.015964672000003 Params:  44549160
======== model resnet img_size 384 params 44549160 flops 23.015964672000003
01/17 11:09:17 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 33 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 40.74766336), ('batch_norm', 0.169607168)])
GFlops:  40.917270527999996 Params:  44549160
======== model resnet img_size 512 params 44549160 flops 40.917270527999996
01/17 11:09:25 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 33 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 63.668224), ('batch_norm', 0.2650112)])
GFlops:  63.9332352 Params:  44549160
======== model resnet img_size 640 params 44549160 flops 63.9332352
01/17 11:09:34 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 33 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 91.68224256), ('batch_norm', 0.381616128)])
GFlops:  92.06385868800001 Params:  44549160
======== model resnet img_size 768 params 44549160 flops 92.06385868800001
01/17 11:09:43 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 33 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 162.99065344), ('batch_norm', 0.678428672)])
GFlops:  163.66908211199998 Params:  44549160
======== model resnet img_size 1024 params 44549160 flops 163.66908211199998
01/17 11:09:52 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 33 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 194.983936), ('batch_norm', 0.8115968)])
GFlops:  195.7955328 Params:  44549160
======== model resnet img_size 1120 params 44549160 flops 195.7955328
01/17 11:10:02 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/resnet/resnet101_8xb32_in1k_20210831-539c63f8.pth
Unsupported operator aten::max_pool2d encountered 1 time(s)
Unsupported operator aten::add_ encountered 33 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 254.672896), ('batch_norm', 1.0600448)])
GFlops:  255.7329408 Params:  44549160
======== model resnet img_size 1280 params 44549160 flops 255.7329408
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
============ base ===================
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmbase/ckpt_epoch_260.pth
dict_items([('conv', 0.007471104), ('layer_norm', 0.00538624), ('linear', 0.855638016), ('einsum', 0.2222935), ('PythonOp.SelectiveScanFn', 0.14943246)])
GFlops:  1.2402213199999998 Params:  75229056
======== model vssm img_size 64 params 75229056 flops 1.2402213199999998
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmbase/ckpt_epoch_260.pth
Warning, x.shape torch.Size([1, 7, 7, 512]) is not match even ===========
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Warning, x.shape torch.Size([1, 7, 7, 512]) is not match even ===========
Warning, x.shape torch.Size([1, 7, 7, 512]) is not match even ===========
dict_items([('conv', 0.022760448), ('layer_norm', 0.0163456), ('linear', 2.572681216), ('einsum', 0.672357), ('PythonOp.SelectiveScanFn', 0.454603574)])
GFlops:  3.7387478379999997 Params:  75229056
======== model vssm img_size 112 params 75229056 flops 3.7387478379999997
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmbase/ckpt_epoch_260.pth
dict_items([('conv', 0.091521024), ('layer_norm', 0.06598144), ('linear', 10.481565696), ('einsum', 2.72364), ('PythonOp.SelectiveScanFn', 1.83061146)])
GFlops:  15.19331962 Params:  75229056
======== model vssm img_size 224 params 75229056 flops 15.19331962
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmbase/ckpt_epoch_260.pth
dict_items([('conv', 0.268959744), ('layer_norm', 0.19390464), ('linear', 30.802968576), ('einsum', 8.003), ('PythonOp.SelectiveScanFn', 5.37950476)])
GFlops:  44.64833772 Params:  75229056
======== model vssm img_size 384 params 75229056 flops 44.64833772
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmbase/ckpt_epoch_260.pth
dict_items([('conv', 0.478150656), ('layer_norm', 0.34471936), ('linear', 54.760833024), ('einsum', 14.22675), ('PythonOp.SelectiveScanFn', 9.56315124)])
GFlops:  79.37360428 Params:  75229056
======== model vssm img_size 512 params 75229056 flops 79.37360428
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmbase/ckpt_epoch_260.pth
dict_items([('conv', 0.7471104), ('layer_norm', 0.538624), ('linear', 85.5638016), ('einsum', 22.22935), ('PythonOp.SelectiveScanFn', 14.943246)])
GFlops:  124.022132 Params:  75229056
======== model vssm img_size 640 params 75229056 flops 124.022132
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmbase/ckpt_epoch_260.pth
dict_items([('conv', 1.075838976), ('layer_norm', 0.77561856), ('linear', 123.211874304), ('einsum', 32.012), ('PythonOp.SelectiveScanFn', 21.51803904)])
GFlops:  178.59337087999998 Params:  75229056
======== model vssm img_size 768 params 75229056 flops 178.59337087999998
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmbase/ckpt_epoch_260.pth
dict_items([('conv', 1.912602624), ('layer_norm', 1.37887744), ('linear', 219.043332096), ('einsum', 56.9065), ('PythonOp.SelectiveScanFn', 38.25367496)])
GFlops:  317.49498712 Params:  75229056
======== model vssm img_size 1024 params 75229056 flops 317.49498712
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmbase/ckpt_epoch_260.pth
dict_items([('conv', 2.2880256), ('layer_norm', 1.649536), ('linear', 262.0391424), ('einsum', 68.088), ('PythonOp.SelectiveScanFn', 45.7638615)])
GFlops:  379.8285655 Params:  75229056
======== model vssm img_size 1120 params 75229056 flops 379.8285655
Unsupported operator aten::mul encountered 69 time(s)
Unsupported operator aten::add encountered 132 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
layers.0.blocks.0.drop_path, layers.0.blocks.1.drop_path, layers.1.blocks.0.drop_path, layers.1.blocks.1.drop_path, layers.2.blocks.0.drop_path, layers.2.blocks.1.drop_path, layers.2.blocks.10.drop_path, layers.2.blocks.11.drop_path, layers.2.blocks.12.drop_path, layers.2.blocks.13.drop_path, layers.2.blocks.14.drop_path, layers.2.blocks.15.drop_path, layers.2.blocks.16.drop_path, layers.2.blocks.17.drop_path, layers.2.blocks.18.drop_path, layers.2.blocks.19.drop_path, layers.2.blocks.2.drop_path, layers.2.blocks.20.drop_path, layers.2.blocks.21.drop_path, layers.2.blocks.22.drop_path, layers.2.blocks.23.drop_path, layers.2.blocks.24.drop_path, layers.2.blocks.25.drop_path, layers.2.blocks.26.drop_path, layers.2.blocks.3.drop_path, layers.2.blocks.4.drop_path, layers.2.blocks.5.drop_path, layers.2.blocks.6.drop_path, layers.2.blocks.7.drop_path, layers.2.blocks.8.drop_path, layers.2.blocks.9.drop_path, layers.3.blocks.0.drop_path, layers.3.blocks.1.drop_path
Successfully load ckpt /media/Disk2/LiuYue/Visualize/analyze/../../ckpts/vssmbase/ckpt_epoch_260.pth
dict_items([('conv', 2.9884416), ('layer_norm', 2.154496), ('linear', 342.2552064), ('einsum', 88.9185), ('PythonOp.SelectiveScanFn', 59.771714)])
GFlops:  496.08835799999997 Params:  75229056
======== model vssm img_size 1280 params 75229056 flops 496.08835799999997
01/17 11:14:07 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:14:10 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth
01/17 11:14:13 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:14:15 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth
01/17 11:14:19 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:14:21 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 95 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 99 time(s)
Unsupported operator aten::sub encountered 23 time(s)
Unsupported operator aten::ne encountered 11 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.019267584), ('layer_norm', 0.03687936), ('linear', 15.105785856), ('matmul', 0.304869376)])
GFlops:  15.466802176 Params:  87768224
======== model swin img_size 224 params 87768224 flops 15.466802176
01/17 11:14:39 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:14:42 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.056623104), ('layer_norm', 0.10838016), ('linear', 48.856301568), ('matmul', 1.108839424)])
GFlops:  50.130144256 Params:  87768224
======== model swin img_size 384 params 87768224 flops 50.130144256
01/17 11:14:57 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:15:00 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.100663296), ('layer_norm', 0.19267584), ('linear', 84.858241024), ('matmul', 1.884535296)])
GFlops:  87.03611545599999 Params:  87768224
======== model swin img_size 512 params 87768224 flops 87.03611545599999
01/17 11:15:19 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:15:23 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.1572864), ('layer_norm', 0.301056), ('linear', 127.13787392), ('matmul', 2.68604672)])
GFlops:  130.28226304 Params:  87768224
======== model swin img_size 640 params 87768224 flops 130.28226304
01/17 11:15:42 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:15:44 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.226492416), ('layer_norm', 0.43352064), ('linear', 181.552545792), ('matmul', 3.771529216)])
GFlops:  185.984088064 Params:  87768224
======== model swin img_size 768 params 87768224 flops 185.984088064
01/17 11:15:55 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:15:59 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.402653184), ('layer_norm', 0.77070336), ('linear', 333.427900416), ('matmul', 7.241876992)])
GFlops:  341.843133952 Params:  87768224
======== model swin img_size 1024 params 87768224 flops 341.843133952
01/17 11:16:18 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:16:22 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.4816896), ('layer_norm', 0.921984), ('linear', 377.6446464), ('matmul', 7.6217344)])
GFlops:  386.67005439999997 Params:  87768224
======== model swin img_size 1120 params 87768224 flops 386.67005439999997
01/17 11:16:37 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:16:40 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/swin-transformer/swin_base_224_b16x64_300e_imagenet_20210616_190742-93230b0d.pth
Unsupported operator aten::rsub encountered 48 time(s)
Unsupported operator aten::pad encountered 24 time(s)
Unsupported operator aten::mul encountered 48 time(s)
Unsupported operator aten::add encountered 96 time(s)
Unsupported operator aten::softmax encountered 24 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
Unsupported operator aten::fill_ encountered 108 time(s)
Unsupported operator aten::sub encountered 24 time(s)
Unsupported operator aten::ne encountered 12 time(s)
Unsupported operator aten::im2col encountered 3 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.patch_embed.adaptive_padding, backbone.stages.0.blocks.0.attn.drop, backbone.stages.0.blocks.0.ffn.dropout_layer, backbone.stages.0.blocks.1.attn.drop, backbone.stages.0.blocks.1.ffn.dropout_layer, backbone.stages.0.downsample.adaptive_padding, backbone.stages.1.blocks.0.attn.drop, backbone.stages.1.blocks.0.ffn.dropout_layer, backbone.stages.1.blocks.1.attn.drop, backbone.stages.1.blocks.1.ffn.dropout_layer, backbone.stages.1.downsample.adaptive_padding, backbone.stages.2.blocks.0.attn.drop, backbone.stages.2.blocks.0.ffn.dropout_layer, backbone.stages.2.blocks.1.attn.drop, backbone.stages.2.blocks.1.ffn.dropout_layer, backbone.stages.2.blocks.10.attn.drop, backbone.stages.2.blocks.10.ffn.dropout_layer, backbone.stages.2.blocks.11.attn.drop, backbone.stages.2.blocks.11.ffn.dropout_layer, backbone.stages.2.blocks.12.attn.drop, backbone.stages.2.blocks.12.ffn.dropout_layer, backbone.stages.2.blocks.13.attn.drop, backbone.stages.2.blocks.13.ffn.dropout_layer, backbone.stages.2.blocks.14.attn.drop, backbone.stages.2.blocks.14.ffn.dropout_layer, backbone.stages.2.blocks.15.attn.drop, backbone.stages.2.blocks.15.ffn.dropout_layer, backbone.stages.2.blocks.16.attn.drop, backbone.stages.2.blocks.16.ffn.dropout_layer, backbone.stages.2.blocks.17.attn.drop, backbone.stages.2.blocks.17.ffn.dropout_layer, backbone.stages.2.blocks.2.attn.drop, backbone.stages.2.blocks.2.ffn.dropout_layer, backbone.stages.2.blocks.3.attn.drop, backbone.stages.2.blocks.3.ffn.dropout_layer, backbone.stages.2.blocks.4.attn.drop, backbone.stages.2.blocks.4.ffn.dropout_layer, backbone.stages.2.blocks.5.attn.drop, backbone.stages.2.blocks.5.ffn.dropout_layer, backbone.stages.2.blocks.6.attn.drop, backbone.stages.2.blocks.6.ffn.dropout_layer, backbone.stages.2.blocks.7.attn.drop, backbone.stages.2.blocks.7.ffn.dropout_layer, backbone.stages.2.blocks.8.attn.drop, backbone.stages.2.blocks.8.ffn.dropout_layer, backbone.stages.2.blocks.9.attn.drop, backbone.stages.2.blocks.9.ffn.dropout_layer, backbone.stages.2.downsample.adaptive_padding, backbone.stages.3.blocks.0.attn.drop, backbone.stages.3.blocks.0.ffn.dropout_layer, backbone.stages.3.blocks.1.attn.drop, backbone.stages.3.blocks.1.ffn.dropout_layer, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce, neck, neck.gap
dict_items([('conv', 0.6291456), ('layer_norm', 1.204224), ('linear', 507.344060416), ('matmul', 10.628631552)])
GFlops:  519.806061568 Params:  87768224
======== model swin img_size 1280 params 87768224 flops 519.806061568
01/17 11:16:58 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:16:59 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_32xb128_in1k_20221207-fbdb5eb9.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.04540416), ('layer_norm', 0.00237568), ('linear', 1.207959552)])
GFlops:  1.255739392 Params:  88591464
======== model convnext img_size 64 params 88591464 flops 1.255739392
01/17 11:17:03 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:17:06 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_32xb128_in1k_20221207-fbdb5eb9.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.13174528), ('layer_norm', 0.00720896), ('linear', 3.6175872)])
GFlops:  3.75654144 Params:  88591464
======== model convnext img_size 112 params 88591464 flops 3.75654144
01/17 11:17:10 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:17:12 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_32xb128_in1k_20221207-fbdb5eb9.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 0.55620096), ('layer_norm', 0.02910208), ('linear', 14.797504512)])
GFlops:  15.382807552 Params:  88591464
======== model convnext img_size 224 params 88591464 flops 15.382807552
01/17 11:17:16 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:17:20 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_32xb128_in1k_20221207-fbdb5eb9.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 1.63454976), ('layer_norm', 0.08552448), ('linear', 43.486543872)])
GFlops:  45.206618112 Params:  88591464
======== model convnext img_size 384 params 88591464 flops 45.206618112
01/17 11:17:23 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:17:27 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_32xb128_in1k_20221207-fbdb5eb9.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 2.90586624), ('layer_norm', 0.15204352), ('linear', 77.309411328)])
GFlops:  80.367321088 Params:  88591464
======== model convnext img_size 512 params 88591464 flops 80.367321088
01/17 11:17:34 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:17:38 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_32xb128_in1k_20221207-fbdb5eb9.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 4.540416), ('layer_norm', 0.237568), ('linear', 120.7959552)])
GFlops:  125.5739392 Params:  88591464
======== model convnext img_size 640 params 88591464 flops 125.5739392
01/17 11:17:42 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:17:44 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_32xb128_in1k_20221207-fbdb5eb9.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 6.53819904), ('layer_norm', 0.34209792), ('linear', 173.946175488)])
GFlops:  180.826472448 Params:  88591464
======== model convnext img_size 768 params 88591464 flops 180.826472448
01/17 11:17:49 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:17:52 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_32xb128_in1k_20221207-fbdb5eb9.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 11.62346496), ('layer_norm', 0.60817408), ('linear', 309.237645312)])
GFlops:  321.469284352 Params:  88591464
======== model convnext img_size 1024 params 88591464 flops 321.469284352
01/17 11:17:57 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:18:01 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_32xb128_in1k_20221207-fbdb5eb9.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 13.905024), ('layer_norm', 0.727552), ('linear', 369.9376128)])
GFlops:  384.5701888 Params:  88591464
======== model convnext img_size 1120 params 88591464 flops 384.5701888
01/17 11:18:06 - mmengine - INFO - Because batch augmentations are enabled, the data preprocessor automatically enables the `to_onehot` option to generate one-hot format labels.
01/17 11:18:09 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/convnext/convnext-base_32xb128_in1k_20221207-fbdb5eb9.pth
Unsupported operator aten::gelu encountered 36 time(s)
Unsupported operator aten::mul encountered 36 time(s)
Unsupported operator aten::add encountered 36 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.1.drop_path, backbone.stages.0.2.drop_path, backbone.stages.1.0.drop_path, backbone.stages.1.1.drop_path, backbone.stages.1.2.drop_path, backbone.stages.2.0.drop_path, backbone.stages.2.1.drop_path, backbone.stages.2.10.drop_path, backbone.stages.2.11.drop_path, backbone.stages.2.12.drop_path, backbone.stages.2.13.drop_path, backbone.stages.2.14.drop_path, backbone.stages.2.15.drop_path, backbone.stages.2.16.drop_path, backbone.stages.2.17.drop_path, backbone.stages.2.18.drop_path, backbone.stages.2.19.drop_path, backbone.stages.2.2.drop_path, backbone.stages.2.20.drop_path, backbone.stages.2.21.drop_path, backbone.stages.2.22.drop_path, backbone.stages.2.23.drop_path, backbone.stages.2.24.drop_path, backbone.stages.2.25.drop_path, backbone.stages.2.26.drop_path, backbone.stages.2.3.drop_path, backbone.stages.2.4.drop_path, backbone.stages.2.5.drop_path, backbone.stages.2.6.drop_path, backbone.stages.2.7.drop_path, backbone.stages.2.8.drop_path, backbone.stages.2.9.drop_path, backbone.stages.3.0.drop_path, backbone.stages.3.1.drop_path, backbone.stages.3.2.drop_path, data_preprocessor, head, head.fc, head.loss_module, head.loss_module.ce
dict_items([('conv', 18.161664), ('layer_norm', 0.950272), ('linear', 483.1838208)])
GFlops:  502.2957568 Params:  88591464
======== model convnext img_size 1280 params 88591464 flops 502.2957568
01/17 11:18:16 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-fd08e268.pth
Unsupported operator aten::add_ encountered 24 time(s)
Unsupported operator aten::add encountered 48 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.blocks.2.drop_path, backbone.stages.0.blocks.3.drop_path, backbone.stages.1.blocks.0.drop_path, backbone.stages.1.blocks.1.drop_path, backbone.stages.1.blocks.2.drop_path, backbone.stages.1.blocks.3.drop_path, backbone.stages.2.blocks.0.drop_path, backbone.stages.2.blocks.1.drop_path, backbone.stages.2.blocks.10.drop_path, backbone.stages.2.blocks.11.drop_path, backbone.stages.2.blocks.12.drop_path, backbone.stages.2.blocks.13.drop_path, backbone.stages.2.blocks.14.drop_path, backbone.stages.2.blocks.15.drop_path, backbone.stages.2.blocks.16.drop_path, backbone.stages.2.blocks.17.drop_path, backbone.stages.2.blocks.18.drop_path, backbone.stages.2.blocks.19.drop_path, backbone.stages.2.blocks.2.drop_path, backbone.stages.2.blocks.20.drop_path, backbone.stages.2.blocks.21.drop_path, backbone.stages.2.blocks.22.drop_path, backbone.stages.2.blocks.23.drop_path, backbone.stages.2.blocks.24.drop_path, backbone.stages.2.blocks.25.drop_path, backbone.stages.2.blocks.26.drop_path, backbone.stages.2.blocks.27.drop_path, backbone.stages.2.blocks.28.drop_path, backbone.stages.2.blocks.29.drop_path, backbone.stages.2.blocks.3.drop_path, backbone.stages.2.blocks.30.drop_path, backbone.stages.2.blocks.31.drop_path, backbone.stages.2.blocks.32.drop_path, backbone.stages.2.blocks.33.drop_path, backbone.stages.2.blocks.34.drop_path, backbone.stages.2.blocks.35.drop_path, backbone.stages.2.blocks.4.drop_path, backbone.stages.2.blocks.5.drop_path, backbone.stages.2.blocks.6.drop_path, backbone.stages.2.blocks.7.drop_path, backbone.stages.2.blocks.8.drop_path, backbone.stages.2.blocks.9.drop_path, backbone.stages.3.blocks.0.drop_path, backbone.stages.3.blocks.1.drop_path, backbone.stages.3.blocks.2.drop_path, backbone.stages.3.blocks.3.drop_path, data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 1.259614208), ('batch_norm', 0.006733824)])
GFlops:  1.266348032 Params:  79864168
======== model replknet img_size 64 params 79864168 flops 1.266348032
01/17 11:18:37 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-fd08e268.pth
Unsupported operator aten::add_ encountered 24 time(s)
Unsupported operator aten::add encountered 48 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.blocks.2.drop_path, backbone.stages.0.blocks.3.drop_path, backbone.stages.1.blocks.0.drop_path, backbone.stages.1.blocks.1.drop_path, backbone.stages.1.blocks.2.drop_path, backbone.stages.1.blocks.3.drop_path, backbone.stages.2.blocks.0.drop_path, backbone.stages.2.blocks.1.drop_path, backbone.stages.2.blocks.10.drop_path, backbone.stages.2.blocks.11.drop_path, backbone.stages.2.blocks.12.drop_path, backbone.stages.2.blocks.13.drop_path, backbone.stages.2.blocks.14.drop_path, backbone.stages.2.blocks.15.drop_path, backbone.stages.2.blocks.16.drop_path, backbone.stages.2.blocks.17.drop_path, backbone.stages.2.blocks.18.drop_path, backbone.stages.2.blocks.19.drop_path, backbone.stages.2.blocks.2.drop_path, backbone.stages.2.blocks.20.drop_path, backbone.stages.2.blocks.21.drop_path, backbone.stages.2.blocks.22.drop_path, backbone.stages.2.blocks.23.drop_path, backbone.stages.2.blocks.24.drop_path, backbone.stages.2.blocks.25.drop_path, backbone.stages.2.blocks.26.drop_path, backbone.stages.2.blocks.27.drop_path, backbone.stages.2.blocks.28.drop_path, backbone.stages.2.blocks.29.drop_path, backbone.stages.2.blocks.3.drop_path, backbone.stages.2.blocks.30.drop_path, backbone.stages.2.blocks.31.drop_path, backbone.stages.2.blocks.32.drop_path, backbone.stages.2.blocks.33.drop_path, backbone.stages.2.blocks.34.drop_path, backbone.stages.2.blocks.35.drop_path, backbone.stages.2.blocks.4.drop_path, backbone.stages.2.blocks.5.drop_path, backbone.stages.2.blocks.6.drop_path, backbone.stages.2.blocks.7.drop_path, backbone.stages.2.blocks.8.drop_path, backbone.stages.2.blocks.9.drop_path, backbone.stages.3.blocks.0.drop_path, backbone.stages.3.blocks.1.drop_path, backbone.stages.3.blocks.2.drop_path, backbone.stages.3.blocks.3.drop_path, data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 3.937736192), ('batch_norm', 0.020806656)])
GFlops:  3.958542848 Params:  79864168
======== model replknet img_size 112 params 79864168 flops 3.958542848
01/17 11:18:46 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-fd08e268.pth
Unsupported operator aten::add_ encountered 24 time(s)
Unsupported operator aten::add encountered 48 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.blocks.2.drop_path, backbone.stages.0.blocks.3.drop_path, backbone.stages.1.blocks.0.drop_path, backbone.stages.1.blocks.1.drop_path, backbone.stages.1.blocks.2.drop_path, backbone.stages.1.blocks.3.drop_path, backbone.stages.2.blocks.0.drop_path, backbone.stages.2.blocks.1.drop_path, backbone.stages.2.blocks.10.drop_path, backbone.stages.2.blocks.11.drop_path, backbone.stages.2.blocks.12.drop_path, backbone.stages.2.blocks.13.drop_path, backbone.stages.2.blocks.14.drop_path, backbone.stages.2.blocks.15.drop_path, backbone.stages.2.blocks.16.drop_path, backbone.stages.2.blocks.17.drop_path, backbone.stages.2.blocks.18.drop_path, backbone.stages.2.blocks.19.drop_path, backbone.stages.2.blocks.2.drop_path, backbone.stages.2.blocks.20.drop_path, backbone.stages.2.blocks.21.drop_path, backbone.stages.2.blocks.22.drop_path, backbone.stages.2.blocks.23.drop_path, backbone.stages.2.blocks.24.drop_path, backbone.stages.2.blocks.25.drop_path, backbone.stages.2.blocks.26.drop_path, backbone.stages.2.blocks.27.drop_path, backbone.stages.2.blocks.28.drop_path, backbone.stages.2.blocks.29.drop_path, backbone.stages.2.blocks.3.drop_path, backbone.stages.2.blocks.30.drop_path, backbone.stages.2.blocks.31.drop_path, backbone.stages.2.blocks.32.drop_path, backbone.stages.2.blocks.33.drop_path, backbone.stages.2.blocks.34.drop_path, backbone.stages.2.blocks.35.drop_path, backbone.stages.2.blocks.4.drop_path, backbone.stages.2.blocks.5.drop_path, backbone.stages.2.blocks.6.drop_path, backbone.stages.2.blocks.7.drop_path, backbone.stages.2.blocks.8.drop_path, backbone.stages.2.blocks.9.drop_path, backbone.stages.3.blocks.0.drop_path, backbone.stages.3.blocks.1.drop_path, backbone.stages.3.blocks.2.drop_path, backbone.stages.3.blocks.3.drop_path, data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 15.430274048), ('batch_norm', 0.082489344)])
GFlops:  15.512763392 Params:  79864168
======== model replknet img_size 224 params 79864168 flops 15.512763392
01/17 11:19:05 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-fd08e268.pth
Unsupported operator aten::add_ encountered 24 time(s)
Unsupported operator aten::add encountered 48 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.blocks.2.drop_path, backbone.stages.0.blocks.3.drop_path, backbone.stages.1.blocks.0.drop_path, backbone.stages.1.blocks.1.drop_path, backbone.stages.1.blocks.2.drop_path, backbone.stages.1.blocks.3.drop_path, backbone.stages.2.blocks.0.drop_path, backbone.stages.2.blocks.1.drop_path, backbone.stages.2.blocks.10.drop_path, backbone.stages.2.blocks.11.drop_path, backbone.stages.2.blocks.12.drop_path, backbone.stages.2.blocks.13.drop_path, backbone.stages.2.blocks.14.drop_path, backbone.stages.2.blocks.15.drop_path, backbone.stages.2.blocks.16.drop_path, backbone.stages.2.blocks.17.drop_path, backbone.stages.2.blocks.18.drop_path, backbone.stages.2.blocks.19.drop_path, backbone.stages.2.blocks.2.drop_path, backbone.stages.2.blocks.20.drop_path, backbone.stages.2.blocks.21.drop_path, backbone.stages.2.blocks.22.drop_path, backbone.stages.2.blocks.23.drop_path, backbone.stages.2.blocks.24.drop_path, backbone.stages.2.blocks.25.drop_path, backbone.stages.2.blocks.26.drop_path, backbone.stages.2.blocks.27.drop_path, backbone.stages.2.blocks.28.drop_path, backbone.stages.2.blocks.29.drop_path, backbone.stages.2.blocks.3.drop_path, backbone.stages.2.blocks.30.drop_path, backbone.stages.2.blocks.31.drop_path, backbone.stages.2.blocks.32.drop_path, backbone.stages.2.blocks.33.drop_path, backbone.stages.2.blocks.34.drop_path, backbone.stages.2.blocks.35.drop_path, backbone.stages.2.blocks.4.drop_path, backbone.stages.2.blocks.5.drop_path, backbone.stages.2.blocks.6.drop_path, backbone.stages.2.blocks.7.drop_path, backbone.stages.2.blocks.8.drop_path, backbone.stages.2.blocks.9.drop_path, backbone.stages.3.blocks.0.drop_path, backbone.stages.3.blocks.1.drop_path, backbone.stages.3.blocks.2.drop_path, backbone.stages.3.blocks.3.drop_path, data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 45.346111488), ('batch_norm', 0.242417664)])
GFlops:  45.588529152 Params:  79864168
======== model replknet img_size 384 params 79864168 flops 45.588529152
01/17 11:19:28 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-fd08e268.pth
Unsupported operator aten::add_ encountered 24 time(s)
Unsupported operator aten::add encountered 48 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.blocks.2.drop_path, backbone.stages.0.blocks.3.drop_path, backbone.stages.1.blocks.0.drop_path, backbone.stages.1.blocks.1.drop_path, backbone.stages.1.blocks.2.drop_path, backbone.stages.1.blocks.3.drop_path, backbone.stages.2.blocks.0.drop_path, backbone.stages.2.blocks.1.drop_path, backbone.stages.2.blocks.10.drop_path, backbone.stages.2.blocks.11.drop_path, backbone.stages.2.blocks.12.drop_path, backbone.stages.2.blocks.13.drop_path, backbone.stages.2.blocks.14.drop_path, backbone.stages.2.blocks.15.drop_path, backbone.stages.2.blocks.16.drop_path, backbone.stages.2.blocks.17.drop_path, backbone.stages.2.blocks.18.drop_path, backbone.stages.2.blocks.19.drop_path, backbone.stages.2.blocks.2.drop_path, backbone.stages.2.blocks.20.drop_path, backbone.stages.2.blocks.21.drop_path, backbone.stages.2.blocks.22.drop_path, backbone.stages.2.blocks.23.drop_path, backbone.stages.2.blocks.24.drop_path, backbone.stages.2.blocks.25.drop_path, backbone.stages.2.blocks.26.drop_path, backbone.stages.2.blocks.27.drop_path, backbone.stages.2.blocks.28.drop_path, backbone.stages.2.blocks.29.drop_path, backbone.stages.2.blocks.3.drop_path, backbone.stages.2.blocks.30.drop_path, backbone.stages.2.blocks.31.drop_path, backbone.stages.2.blocks.32.drop_path, backbone.stages.2.blocks.33.drop_path, backbone.stages.2.blocks.34.drop_path, backbone.stages.2.blocks.35.drop_path, backbone.stages.2.blocks.4.drop_path, backbone.stages.2.blocks.5.drop_path, backbone.stages.2.blocks.6.drop_path, backbone.stages.2.blocks.7.drop_path, backbone.stages.2.blocks.8.drop_path, backbone.stages.2.blocks.9.drop_path, backbone.stages.3.blocks.0.drop_path, backbone.stages.3.blocks.1.drop_path, backbone.stages.3.blocks.2.drop_path, backbone.stages.3.blocks.3.drop_path, data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 80.615309312), ('batch_norm', 0.430964736)])
GFlops:  81.046274048 Params:  79864168
======== model replknet img_size 512 params 79864168 flops 81.046274048
01/17 11:19:47 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-fd08e268.pth
Unsupported operator aten::add_ encountered 24 time(s)
Unsupported operator aten::add encountered 48 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.blocks.2.drop_path, backbone.stages.0.blocks.3.drop_path, backbone.stages.1.blocks.0.drop_path, backbone.stages.1.blocks.1.drop_path, backbone.stages.1.blocks.2.drop_path, backbone.stages.1.blocks.3.drop_path, backbone.stages.2.blocks.0.drop_path, backbone.stages.2.blocks.1.drop_path, backbone.stages.2.blocks.10.drop_path, backbone.stages.2.blocks.11.drop_path, backbone.stages.2.blocks.12.drop_path, backbone.stages.2.blocks.13.drop_path, backbone.stages.2.blocks.14.drop_path, backbone.stages.2.blocks.15.drop_path, backbone.stages.2.blocks.16.drop_path, backbone.stages.2.blocks.17.drop_path, backbone.stages.2.blocks.18.drop_path, backbone.stages.2.blocks.19.drop_path, backbone.stages.2.blocks.2.drop_path, backbone.stages.2.blocks.20.drop_path, backbone.stages.2.blocks.21.drop_path, backbone.stages.2.blocks.22.drop_path, backbone.stages.2.blocks.23.drop_path, backbone.stages.2.blocks.24.drop_path, backbone.stages.2.blocks.25.drop_path, backbone.stages.2.blocks.26.drop_path, backbone.stages.2.blocks.27.drop_path, backbone.stages.2.blocks.28.drop_path, backbone.stages.2.blocks.29.drop_path, backbone.stages.2.blocks.3.drop_path, backbone.stages.2.blocks.30.drop_path, backbone.stages.2.blocks.31.drop_path, backbone.stages.2.blocks.32.drop_path, backbone.stages.2.blocks.33.drop_path, backbone.stages.2.blocks.34.drop_path, backbone.stages.2.blocks.35.drop_path, backbone.stages.2.blocks.4.drop_path, backbone.stages.2.blocks.5.drop_path, backbone.stages.2.blocks.6.drop_path, backbone.stages.2.blocks.7.drop_path, backbone.stages.2.blocks.8.drop_path, backbone.stages.2.blocks.9.drop_path, backbone.stages.3.blocks.0.drop_path, backbone.stages.3.blocks.1.drop_path, backbone.stages.3.blocks.2.drop_path, backbone.stages.3.blocks.3.drop_path, data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 125.9614208), ('batch_norm', 0.6733824)])
GFlops:  126.6348032 Params:  79864168
======== model replknet img_size 640 params 79864168 flops 126.6348032
01/17 11:20:08 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-fd08e268.pth
Unsupported operator aten::add_ encountered 24 time(s)
Unsupported operator aten::add encountered 48 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.blocks.2.drop_path, backbone.stages.0.blocks.3.drop_path, backbone.stages.1.blocks.0.drop_path, backbone.stages.1.blocks.1.drop_path, backbone.stages.1.blocks.2.drop_path, backbone.stages.1.blocks.3.drop_path, backbone.stages.2.blocks.0.drop_path, backbone.stages.2.blocks.1.drop_path, backbone.stages.2.blocks.10.drop_path, backbone.stages.2.blocks.11.drop_path, backbone.stages.2.blocks.12.drop_path, backbone.stages.2.blocks.13.drop_path, backbone.stages.2.blocks.14.drop_path, backbone.stages.2.blocks.15.drop_path, backbone.stages.2.blocks.16.drop_path, backbone.stages.2.blocks.17.drop_path, backbone.stages.2.blocks.18.drop_path, backbone.stages.2.blocks.19.drop_path, backbone.stages.2.blocks.2.drop_path, backbone.stages.2.blocks.20.drop_path, backbone.stages.2.blocks.21.drop_path, backbone.stages.2.blocks.22.drop_path, backbone.stages.2.blocks.23.drop_path, backbone.stages.2.blocks.24.drop_path, backbone.stages.2.blocks.25.drop_path, backbone.stages.2.blocks.26.drop_path, backbone.stages.2.blocks.27.drop_path, backbone.stages.2.blocks.28.drop_path, backbone.stages.2.blocks.29.drop_path, backbone.stages.2.blocks.3.drop_path, backbone.stages.2.blocks.30.drop_path, backbone.stages.2.blocks.31.drop_path, backbone.stages.2.blocks.32.drop_path, backbone.stages.2.blocks.33.drop_path, backbone.stages.2.blocks.34.drop_path, backbone.stages.2.blocks.35.drop_path, backbone.stages.2.blocks.4.drop_path, backbone.stages.2.blocks.5.drop_path, backbone.stages.2.blocks.6.drop_path, backbone.stages.2.blocks.7.drop_path, backbone.stages.2.blocks.8.drop_path, backbone.stages.2.blocks.9.drop_path, backbone.stages.3.blocks.0.drop_path, backbone.stages.3.blocks.1.drop_path, backbone.stages.3.blocks.2.drop_path, backbone.stages.3.blocks.3.drop_path, data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 181.384445952), ('batch_norm', 0.969670656)])
GFlops:  182.354116608 Params:  79864168
======== model replknet img_size 768 params 79864168 flops 182.354116608
01/17 11:20:28 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-fd08e268.pth
Unsupported operator aten::add_ encountered 24 time(s)
Unsupported operator aten::add encountered 48 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.blocks.2.drop_path, backbone.stages.0.blocks.3.drop_path, backbone.stages.1.blocks.0.drop_path, backbone.stages.1.blocks.1.drop_path, backbone.stages.1.blocks.2.drop_path, backbone.stages.1.blocks.3.drop_path, backbone.stages.2.blocks.0.drop_path, backbone.stages.2.blocks.1.drop_path, backbone.stages.2.blocks.10.drop_path, backbone.stages.2.blocks.11.drop_path, backbone.stages.2.blocks.12.drop_path, backbone.stages.2.blocks.13.drop_path, backbone.stages.2.blocks.14.drop_path, backbone.stages.2.blocks.15.drop_path, backbone.stages.2.blocks.16.drop_path, backbone.stages.2.blocks.17.drop_path, backbone.stages.2.blocks.18.drop_path, backbone.stages.2.blocks.19.drop_path, backbone.stages.2.blocks.2.drop_path, backbone.stages.2.blocks.20.drop_path, backbone.stages.2.blocks.21.drop_path, backbone.stages.2.blocks.22.drop_path, backbone.stages.2.blocks.23.drop_path, backbone.stages.2.blocks.24.drop_path, backbone.stages.2.blocks.25.drop_path, backbone.stages.2.blocks.26.drop_path, backbone.stages.2.blocks.27.drop_path, backbone.stages.2.blocks.28.drop_path, backbone.stages.2.blocks.29.drop_path, backbone.stages.2.blocks.3.drop_path, backbone.stages.2.blocks.30.drop_path, backbone.stages.2.blocks.31.drop_path, backbone.stages.2.blocks.32.drop_path, backbone.stages.2.blocks.33.drop_path, backbone.stages.2.blocks.34.drop_path, backbone.stages.2.blocks.35.drop_path, backbone.stages.2.blocks.4.drop_path, backbone.stages.2.blocks.5.drop_path, backbone.stages.2.blocks.6.drop_path, backbone.stages.2.blocks.7.drop_path, backbone.stages.2.blocks.8.drop_path, backbone.stages.2.blocks.9.drop_path, backbone.stages.3.blocks.0.drop_path, backbone.stages.3.blocks.1.drop_path, backbone.stages.3.blocks.2.drop_path, backbone.stages.3.blocks.3.drop_path, data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 322.461237248), ('batch_norm', 1.723858944)])
GFlops:  324.185096192 Params:  79864168
======== model replknet img_size 1024 params 79864168 flops 324.185096192
01/17 11:20:39 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-fd08e268.pth
Unsupported operator aten::add_ encountered 24 time(s)
Unsupported operator aten::add encountered 48 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.blocks.2.drop_path, backbone.stages.0.blocks.3.drop_path, backbone.stages.1.blocks.0.drop_path, backbone.stages.1.blocks.1.drop_path, backbone.stages.1.blocks.2.drop_path, backbone.stages.1.blocks.3.drop_path, backbone.stages.2.blocks.0.drop_path, backbone.stages.2.blocks.1.drop_path, backbone.stages.2.blocks.10.drop_path, backbone.stages.2.blocks.11.drop_path, backbone.stages.2.blocks.12.drop_path, backbone.stages.2.blocks.13.drop_path, backbone.stages.2.blocks.14.drop_path, backbone.stages.2.blocks.15.drop_path, backbone.stages.2.blocks.16.drop_path, backbone.stages.2.blocks.17.drop_path, backbone.stages.2.blocks.18.drop_path, backbone.stages.2.blocks.19.drop_path, backbone.stages.2.blocks.2.drop_path, backbone.stages.2.blocks.20.drop_path, backbone.stages.2.blocks.21.drop_path, backbone.stages.2.blocks.22.drop_path, backbone.stages.2.blocks.23.drop_path, backbone.stages.2.blocks.24.drop_path, backbone.stages.2.blocks.25.drop_path, backbone.stages.2.blocks.26.drop_path, backbone.stages.2.blocks.27.drop_path, backbone.stages.2.blocks.28.drop_path, backbone.stages.2.blocks.29.drop_path, backbone.stages.2.blocks.3.drop_path, backbone.stages.2.blocks.30.drop_path, backbone.stages.2.blocks.31.drop_path, backbone.stages.2.blocks.32.drop_path, backbone.stages.2.blocks.33.drop_path, backbone.stages.2.blocks.34.drop_path, backbone.stages.2.blocks.35.drop_path, backbone.stages.2.blocks.4.drop_path, backbone.stages.2.blocks.5.drop_path, backbone.stages.2.blocks.6.drop_path, backbone.stages.2.blocks.7.drop_path, backbone.stages.2.blocks.8.drop_path, backbone.stages.2.blocks.9.drop_path, backbone.stages.3.blocks.0.drop_path, backbone.stages.3.blocks.1.drop_path, backbone.stages.3.blocks.2.drop_path, backbone.stages.3.blocks.3.drop_path, data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 385.7568512), ('batch_norm', 2.0622336)])
GFlops:  387.81908480000004 Params:  79864168
======== model replknet img_size 1120 params 79864168 flops 387.81908480000004
01/17 11:20:53 - mmengine - INFO - Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/replknet/replknet-31B_3rdparty_in1k_20221118-fd08e268.pth
Unsupported operator aten::add_ encountered 24 time(s)
Unsupported operator aten::add encountered 48 time(s)
Unsupported operator aten::gelu encountered 24 time(s)
The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
backbone.stages.0.blocks.2.drop_path, backbone.stages.0.blocks.3.drop_path, backbone.stages.1.blocks.0.drop_path, backbone.stages.1.blocks.1.drop_path, backbone.stages.1.blocks.2.drop_path, backbone.stages.1.blocks.3.drop_path, backbone.stages.2.blocks.0.drop_path, backbone.stages.2.blocks.1.drop_path, backbone.stages.2.blocks.10.drop_path, backbone.stages.2.blocks.11.drop_path, backbone.stages.2.blocks.12.drop_path, backbone.stages.2.blocks.13.drop_path, backbone.stages.2.blocks.14.drop_path, backbone.stages.2.blocks.15.drop_path, backbone.stages.2.blocks.16.drop_path, backbone.stages.2.blocks.17.drop_path, backbone.stages.2.blocks.18.drop_path, backbone.stages.2.blocks.19.drop_path, backbone.stages.2.blocks.2.drop_path, backbone.stages.2.blocks.20.drop_path, backbone.stages.2.blocks.21.drop_path, backbone.stages.2.blocks.22.drop_path, backbone.stages.2.blocks.23.drop_path, backbone.stages.2.blocks.24.drop_path, backbone.stages.2.blocks.25.drop_path, backbone.stages.2.blocks.26.drop_path, backbone.stages.2.blocks.27.drop_path, backbone.stages.2.blocks.28.drop_path, backbone.stages.2.blocks.29.drop_path, backbone.stages.2.blocks.3.drop_path, backbone.stages.2.blocks.30.drop_path, backbone.stages.2.blocks.31.drop_path, backbone.stages.2.blocks.32.drop_path, backbone.stages.2.blocks.33.drop_path, backbone.stages.2.blocks.34.drop_path, backbone.stages.2.blocks.35.drop_path, backbone.stages.2.blocks.4.drop_path, backbone.stages.2.blocks.5.drop_path, backbone.stages.2.blocks.6.drop_path, backbone.stages.2.blocks.7.drop_path, backbone.stages.2.blocks.8.drop_path, backbone.stages.2.blocks.9.drop_path, backbone.stages.3.blocks.0.drop_path, backbone.stages.3.blocks.1.drop_path, backbone.stages.3.blocks.2.drop_path, backbone.stages.3.blocks.3.drop_path, data_preprocessor, head, head.fc, head.loss_module, neck, neck.gap
dict_items([('conv', 503.8456832), ('batch_norm', 2.6935296)])
GFlops:  506.5392128 Params:  79864168
======== model replknet img_size 1280 params 79864168 flops 506.5392128
